{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain import OpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "from groq import Groq\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_filings(directory):\n",
    "    filings = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(os.path.join(root, file), 'r') as f:\n",
    "                    filing_text = f.read()\n",
    "                    filings.append(filing_text)\n",
    "    merged_data = \"\\n\".join(filings)\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"C:\\Users\\Amini Sharma\\financial_services_project\\sec-edgar-filings\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge data for AAPL\n",
    "aapl_data = merge_filings(os.path.join(directory, r\"AAPL\\10-K\"))\n",
    "# Merge data for GOOGL\n",
    "tup_data = merge_filings(os.path.join(directory, r\"tup\\10-K\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def llama_crx(data):\n",
    "#     try:\n",
    "#         response = requests.post(\n",
    "#             f\"https://api.groq.com/v1/models/llama3-8b-8192/generate\",\n",
    "#             headers={\"Authorization\": \"Bearer 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\"},\n",
    "#             json={\"prompts\": [data]},\n",
    "#         )\n",
    "#         return response.json()[0]\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "#         return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_crx(data):\n",
    "    try:\n",
    "        # Create a Groq client instance\n",
    "        # Generate a response using the Groq client\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": data,\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=8192\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Request timed out.\n"
     ]
    }
   ],
   "source": [
    "tup_insight = llama_crx(tup_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: <html>\n",
      "<head><title>502 Bad Gateway</title></head>\n",
      "<body>\n",
      "<center><h1>502 Bad Gateway</h1></center>\n",
      "<hr><center>cloudflare</center>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "dbxl_insight = llama_crx(_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googl_insight = llama_crx(googl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344577940\n"
     ]
    }
   ],
   "source": [
    "print(len(msft_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model (LLM) is a type of artificial intelligence (AI) model that is trained on large amounts of text data to generate human-like language understanding and generation capabilities. LLMs are designed to process and generate human language, allowing them to understand and respond to natural language inputs.\n",
      "\n",
      "Key characteristics of Large Language Models:\n",
      "\n",
      "1. **Scale**: LLMs are trained on massive datasets of text, often comprising billions of words, to learn patterns, relationships, and context.\n",
      "2. **Complexity**: LLMs consist of complex neural networks with millions or billions of parameters, allowing them to capture subtle nuances in language.\n",
      "3. **Contextual understanding**: LLMs can comprehend the context, syntax, and semantics of natural language, enabling them to generate coherent and relevant responses.\n",
      "4. **Unsupervised learning**: LLMs are typically trained without explicit supervision, relying on self-supervised learning, reinforcement learning, or generative adversarial networks to improve their performance.\n",
      "\n",
      "Some common applications of Large Language Models:\n",
      "\n",
      "1. **Conversational AI**: LLMs power chatbots, voice assistants, and other conversational dialogue systems.\n",
      "2. **Natural Language Processing (NLP)**: LLMs enable tasks like text classification, sentiment analysis, named entity recognition, and text summarization.\n",
      "3. **Language Translation**: LLMs are used in machine translation systems to translate languages in real-time.\n",
      "4. **Content Generation**: LLMs can generate text, such as articles, blog posts, and even entire books.\n",
      "5. **Research**: LLMs are used in research to study language and understanding, enabling the creation of new AI benchmarks and evaluating the performance of language AI models.\n",
      "\n",
      "Notable examples of Large Language Models:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: A widely used LLM developed by Google that achieved state-of-the-art results in various NLP tasks.\n",
      "2. **RoBERTa**: A variant of BERT that achieved better results on many NLP tasks, particularly in the areas of language modeling and sentiment analysis.\n",
      "3. **Long Short-Term Memory (LSTM) networks**: A type of recurrent neural network (RNN) that can be used to model sequential data, such as text.\n",
      "\n",
      "While LLMs have made significant progress in language understanding and generation, they still face challenges in areas like:\n",
      "\n",
      "1. **Common sense and human intuition**: LLMs often struggle to understand human common sense, humor, and nuanced language.\n",
      "2. **Ambiguity and context**: LLMs may struggle to disambiguate ambiguous text or understand subtle contextual cues.\n",
      "3. **Out-of-distribution data**: LLMs may not generalize well to new, unseen data or unusual linguistic patterns.\n",
      "\n",
      "Despite these challenges, Large Language Models have the potential to revolutionize the way we interact with computers, enabling more intuitive and human-like communication.\n"
     ]
    }
   ],
   "source": [
    "print(llama_crx(\"What is a large language model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
